{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42UBKEnat_xo"
      },
      "source": [
        "# **Identifying Key Entities in Recipe Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pme3h_fduOKh"
      },
      "source": [
        "\n",
        "**Business Objective**:\n",
        "The goal of this assignment is to train a Named Entity Recognition (NER) model using Conditional Random Fields (CRF) to extract key entities from recipe data. The model will classify words into predefined categories such as ingredients, quantities and units, enabling the creation of a structured database of recipes and ingredients that can be used to power advanced features in recipe management systems, dietary tracking apps, or e-commerce platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXzoAs8evNG0"
      },
      "source": [
        "### **Data Description**\n",
        "The given data is in JSON format, representing a **structured recipe ingredient list** with **Named Entity Recognition (NER) labels**. Below is a breakdown of the data fields:\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"input\": \"6 Karela Bitter Gourd Pavakkai Salt 1 Onion 3 tablespoon Gram flour besan 2 teaspoons Turmeric powder Haldi Red Chilli Cumin seeds Jeera Coriander Powder Dhania Amchur Dry Mango Sunflower Oil\",\n",
        "        \"pos\": \"quantity ingredient ingredient ingredient ingredient ingredient quantity ingredient quantity unit ingredient ingredient ingredient quantity unit ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": \"2-1/2 cups rice cooked 3 tomatoes teaspoons BC Belle Bhat powder 1 teaspoon chickpea lentils 1/2 cumin seeds white urad dal mustard green chilli dry red 2 cashew or peanuts 1-1/2 tablespoon oil asafoetida\",\n",
        "      \"pos\": \"quantity unit ingredient ingredient quantity ingredient unit ingredient ingredient ingredient ingredient quantity unit ingredient ingredient quantity ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient quantity ingredient ingredient ingredient quantity unit ingredient ingredient\"\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSDcNvJlwC6N"
      },
      "source": [
        "| **Key**  | **Description**  |\n",
        "|----------|-----------------|\n",
        "| `input`  | Contains a raw ingredient list from a recipe. |\n",
        "| `pos`    | Represents the corresponding part-of-speech (POS) tags or NER labels, identifying quantities, ingredients, and units. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phenosA4se1c"
      },
      "source": [
        "## **1** Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-jQHin3kQX"
      },
      "source": [
        "#### **1.1** Installation of sklearn-crfsuite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPhaJSfCwpfa"
      },
      "source": [
        "sklearn-crfsuite is a Python wrapper for CRFsuite, a fast and efficient implementation of Conditional Random Fields (CRFs). It is designed to integrate seamlessly with scikit-learn for structured prediction tasks such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QawokgQXAMO",
        "outputId": "3c06affd-567e-4258-a308-5ac027bdeb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn_crfsuite==0.5.0\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite==0.5.0)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite==0.5.0) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite==0.5.0) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite==0.5.0) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite==0.5.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite==0.5.0) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite==0.5.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite==0.5.0) (3.6.0)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0\n"
          ]
        }
      ],
      "source": [
        "# installation of sklearn_crfsuite\n",
        "!pip install sklearn_crfsuite==0.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svqZwrHT3rzV"
      },
      "source": [
        "#### **1.2** Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2zLbaB0w1ZH"
      },
      "outputs": [],
      "source": [
        "# Import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hlp-Ln4WsaV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import json  # For handling JSON data\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import re  # For regular expressions (useful for text preprocessing)\n",
        "import matplotlib.pyplot as plt  # For visualisation\n",
        "import seaborn as sns  # For advanced data visualisation\n",
        "import sklearn_crfsuite  # CRF (Conditional Random Fields) implementation for sequence modeling\n",
        "import numpy as np  # For numerical computations\n",
        "# Saving and loading machine learning models\n",
        "import joblib\n",
        "import random\n",
        "import spacy\n",
        "from IPython.display import display, Markdown # For displaying well-formatted output\n",
        "\n",
        "from fractions import Fraction  # For handling fractional values in numerical data\n",
        "# Importing tools for feature engineering and model training\n",
        "from collections import Counter  # For counting occurrences of elements in a list\n",
        "from sklearn.model_selection import train_test_split  # For splitting dataset into train and test sets\n",
        "from sklearn_crfsuite import metrics  # For evaluating CRF models\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3_LR6N_2cli"
      },
      "outputs": [],
      "source": [
        "# Ensure pandas displays full content\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUOu_u0fyMfh"
      },
      "source": [
        "## **2** Data Ingestion and Preparation <font color = red>[25 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ksMVNgeyiLN"
      },
      "source": [
        "#### **2.1** *Read Recipe Data from Dataframe and prepare the data for analysis* <font color = red>[12 marks]</font> <br>\n",
        "Read the data from JSON file, print first five rows and describe the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxn28jL3z4GY"
      },
      "source": [
        "##### **2.1.1** **Define a *load_json_dataframe* function** <font color = red>[7 marks]</font> <br>\n",
        "\n",
        "Define a function that takes path of the ingredient_and_quantity.json file and reads it, convert it into dataframe - df and return it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq6UgUYcPyOL"
      },
      "outputs": [],
      "source": [
        "# define a function to load json file to a dataframe\n",
        "def load_json_dataframe(path):\n",
        "  json_data_df = pd.read_json(path)\n",
        "  return json_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NlhkH_605IA"
      },
      "source": [
        "##### **2.1.2** **Execute the *load_json_dataframe* function** <font color = red>[2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UONMkMsrxdxB"
      },
      "outputs": [],
      "source": [
        "# read the json file by giving the file path and create a dataframe\n",
        "data_path = './ingredient_and_quantity.json'\n",
        "data_df = load_json_dataframe(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1VkDbev3UHP"
      },
      "source": [
        "##### **2.1.3** **Describe the dataframe** <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Print first five rows of dataframe along with dimensions. Display the information of dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZFj2skZxgpl"
      },
      "outputs": [],
      "source": [
        "# display first five rows of the dataframe - df\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7cA28XSx1I1"
      },
      "outputs": [],
      "source": [
        "# print the dimensions of dataframe - df\n",
        "print(\"Dimensions of the dataframe:\", data_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-gsbEhJx2rm"
      },
      "outputs": [],
      "source": [
        "# print the information of the dataframe\n",
        "data_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y18LwoqyFpk"
      },
      "source": [
        "#### **2.2** *Recipe Data Manipulation* <font color = red>[13 marks]</font> <br>\n",
        "Create derived metrics in dataframe and provide insights of the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhNG_XC1r4Qw"
      },
      "source": [
        "##### **2.2.1** **Create input_tokens and pos_tokens columns by splitting the input and pos from the dataframe** <font color = red>[3 marks]</font> <br>\n",
        "Split the input and pos into input_tokens and pos_tokens in the dataframe and display it in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nma6uJwmXUas"
      },
      "outputs": [],
      "source": [
        "# split the input and pos into input_tokens and pos_tokens in the dataframe\n",
        "# Tokenize input\n",
        "data_df['input_tokens'] = data_df['input'].apply(lambda x: x.split())\n",
        "# Tokenize POS\n",
        "data_df['pos_tokens'] = data_df['pos'].apply(lambda x: x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g-ajvFBzaaf"
      },
      "outputs": [],
      "source": [
        "# display first five rows of the dataframe - df\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JtvsBYur-oV"
      },
      "source": [
        "##### **2.2.2** **Provide the length for input_tokens and pos_tokens and validate their length** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create input_length and pos_length columns in the dataframe and validate both the lengths. Check for the rows that are unequal in input and pos length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeVRD2IK1Jrg"
      },
      "outputs": [],
      "source": [
        "# create input_length and pos_length columns for the input_tokens and pos-tokens\n",
        "data_df['input_length'] = data_df['input_tokens'].apply(len)\n",
        "data_df['pos_length'] = data_df['pos_tokens'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPMOlLnz1P1H"
      },
      "outputs": [],
      "source": [
        "# check for the equality of input_length and pos_length in the dataframe\n",
        "data_df['is_length_equal'] = data_df['input_length'] == data_df['pos_length']\n",
        "display(data_df['is_length_equal'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpJQu3JE_P7Z"
      },
      "source": [
        "##### **2.2.3** **Define a unique_labels function and validate the labels in pos_tokens** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a unique_labels function which checks for all the unique pos labels in the recipe & execute it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4aMFCxXO_GJ"
      },
      "outputs": [],
      "source": [
        "# Define a unique_labels function to checks for all the unique pos labels in the recipe & print it\n",
        "def unique_labels(df):\n",
        "  unique_labels = set()\n",
        "  for pos_tokens in df['pos_tokens']:\n",
        "    unique_labels.update(pos_tokens)\n",
        "  return unique_labels\n",
        "\n",
        "all_unique_labels = unique_labels(data_df)\n",
        "print(\"Unique Labels:\", all_unique_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbriClEV9CW5"
      },
      "source": [
        "##### **2.2.3** **Provide the insights seen in the recipe data after validation** <font color = red>[1 marks]</font> <br>\n",
        "\n",
        "Provide the indexes that requires cleaning and formatting in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvNk4yhIHf31"
      },
      "outputs": [],
      "source": [
        "unequal_length_indexes = data_df[data_df['is_length_equal'] == False].index.tolist()\n",
        "print(\"Indexes requiring cleaning:\", unequal_length_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtqtij2-CD2m"
      },
      "source": [
        "##### **2.2.4** **Drop the rows that have invalid data provided in previous cell** <font color = red> [2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaiy1pYWCFPA"
      },
      "outputs": [],
      "source": [
        "# drop the irrelevant recipe data\n",
        "data_df = data_df.drop(unequal_length_indexes)\n",
        "\n",
        "# Display the dimensions of the dataframe after dropping the rows\n",
        "print(\"Dimensions of the dataframe after dropping rows:\", data_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RJEStPSC9PB"
      },
      "source": [
        "##### **2.2.5** **Update the input_length & pos_length in dataframe**<font color = red> [2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjJd7gPI5_ca"
      },
      "outputs": [],
      "source": [
        "# update the input and pos length in input_length and pos_length\n",
        "data_df['input_length'] = data_df['input_tokens'].apply(len)\n",
        "data_df['pos_length'] = data_df['pos_tokens'].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJdYJ2TEDBzd"
      },
      "source": [
        "##### **2.2.6** **Validate the input_length and pos_length by checking unequal rows** <font color = red> [1 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdSsdOPM8aXo"
      },
      "outputs": [],
      "source": [
        "# validate the input length and pos length as input_length and pos_length\n",
        "data_df['is_length_equal'] = data_df['input_length'] == data_df['pos_length']\n",
        "data_df['is_length_equal'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwKLW4em-qMu"
      },
      "source": [
        "## **3** Train Validation Split (70 train - 30 val) <font color = red>[6 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_pJDTVO-71z"
      },
      "source": [
        "#### **3.1** *Perform train and validation split ratio* <font color = red>[6 marks]</font> <br>\n",
        "Split the dataset with the help of input_tokens and pos_tokens and make a ratio of 70:30 split for training and validation datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-64gdDiIy9u"
      },
      "source": [
        "###### **3.1.1** **Split the dataset into train_df and val_df into 70:30 ratio** <font color = red> [1 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W20A_-9E_WOv"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and validation sets\n",
        "test_data_size = 0.3\n",
        "train_df, val_df = train_test_split(data_df, test_size=test_data_size, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUA05_77JRAv"
      },
      "source": [
        "###### **3.1.2** **Print the first five rows of train_df and val_df** <font color = red> [1 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgMZfsbV_XhK"
      },
      "outputs": [],
      "source": [
        "# print the first five rows of train_df\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgtg5WE4_d7h"
      },
      "outputs": [],
      "source": [
        "# print the first five rows of the val_df\n",
        "val_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7prEiaiqI_VZ"
      },
      "source": [
        "###### **3.1.3** **Extract the dataset into train_df and val_df into X_train, X_val, y_train and y_val and display their length** <font color = red> [2 marks]</font> <br>\n",
        "\n",
        "Extract X_train, X_val, y_train and y_val by extracting the list of input_tokens and pos_tokens from train_df and val_df and also display their length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFVnCD71IHXF"
      },
      "outputs": [],
      "source": [
        "# extract the training and validation sets by taking input_tokens and pos_tokens\n",
        "X_train = train_df['input_tokens']\n",
        "y_train = train_df['pos_tokens']\n",
        "X_val = val_df['input_tokens']\n",
        "y_val = val_df['pos_tokens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQPOVz3J_fiq"
      },
      "outputs": [],
      "source": [
        "# validate the shape of training and validation samples\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uicUYglLeiA"
      },
      "source": [
        "###### **3.1.4** **Display the number of unique labels present in y_train** <font color = red> [2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzWtzpdINt6X"
      },
      "outputs": [],
      "source": [
        "# Display the number of unique labels present in y_train\n",
        "unique_labels = set()\n",
        "for pos_tokens in y_train:\n",
        "  unique_labels.update(pos_tokens)\n",
        "print(\"Number of unique labels in y_train:\", len(unique_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFm46QrB4gmj"
      },
      "source": [
        "## **4** Exploratory Recipe Data Analysis on Training Dataset <font color = red>[16 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUWIp0n_NeH6"
      },
      "source": [
        "#### **4.1** *Flatten the lists for input_tokens & pos_tokens* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function **flatten_list** for flattening the structure for input_tokens and pos_tokens. The input parameter passed to this function is a nested list.\n",
        "\n",
        "Initialise the dataset_name with a value ***'Training'***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzcY0gPiOe8o"
      },
      "outputs": [],
      "source": [
        "# flatten the list for nested_list (input_tokens, pos_tokens)\n",
        "def flatten_list(nested_list):\n",
        "    flattened = [item for sublist in nested_list for item in sublist]\n",
        "    return flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXRda29gNBH8"
      },
      "outputs": [],
      "source": [
        "# initialise the dataset_name\n",
        "dataset_name = 'Training'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGOqhd8OOr1E"
      },
      "source": [
        "#### **4.2** *Extract and validate the tokens after using the flattening technique* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function named ***extract_and_validate_tokens*** with parameters dataframe and dataset_name (Training/Validation), validate the length of input_tokens and pos_tokens from dataframe and display first 10 records for both the input_tokens and pos_tokens. Execute this function\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3GMX83xP7ja"
      },
      "outputs": [],
      "source": [
        "# define a extract_and_validate_tokens with parameters (df, dataset_name)\n",
        "# call the flatten_list and apply it on input_tokens and pos_tokens\n",
        "# validate their length and display first 10 records having input and pos tokens\n",
        "def extract_and_validate_tokens(df, dataset_name):\n",
        "    input_tokens = flatten_list(df['input_tokens'])\n",
        "    pos_tokens = flatten_list(df['pos_tokens'])\n",
        "    if len(input_tokens) == len(pos_tokens):\n",
        "        print(f\"\\nFirst 10 records in {dataset_name} dataset:\")\n",
        "        print(\"Input Tokens:\")\n",
        "        print(input_tokens[:10])\n",
        "        print(\"\\nPOS Tokens:\")\n",
        "        print(pos_tokens[:10])\n",
        "        return input_tokens, pos_tokens\n",
        "    else:\n",
        "        print(\"Input and POS tokens have different lengths.\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajdbYMgeLpf9"
      },
      "outputs": [],
      "source": [
        "# extract the tokens and its pos tags\n",
        "train_ingredients, train_units = extract_and_validate_tokens(train_df, dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htZVn5wcQSok"
      },
      "source": [
        "#### **4.3** *Categorise tokens into labels (unit, ingredient, quantity)* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function ***categorize_tokens*** to categorise tokens into ingredients, units and quantities by using extracted tokens in the previous code and return a list of ingredients, units and quantities. Execute this function to get the list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xq0a4L7Quct"
      },
      "outputs": [],
      "source": [
        "# define a categorize_tokens function and provide the tokens and pos_tags as parameters and create ingredient, unit and quantity list and return it\n",
        "# validate the list that it comprised of these labels, if not return empty arrays\n",
        "def categorize_tokens(tokens, pos_tags):\n",
        "    ingredients = []\n",
        "    units = []\n",
        "    quantities = []\n",
        "    # Check if the lengths of tokens and pos_tags are equal before proceeding\n",
        "    if len(tokens) != len(pos_tags):\n",
        "      return [], [], []\n",
        "\n",
        "    for token, pos in zip(tokens, pos_tags):\n",
        "        if pos == 'ingredient':\n",
        "            ingredients.append(token)\n",
        "        elif pos == 'unit':\n",
        "            units.append(token)\n",
        "        elif pos == 'quantity':\n",
        "            quantities.append(token)\n",
        "    return ingredients, units, quantities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evcsigvUL7bM"
      },
      "outputs": [],
      "source": [
        "#  call the function to categorise the labels into respective list\n",
        "train_ingredients_list, train_units_list, train_quantities_list = categorize_tokens(train_ingredients, train_units)\n",
        "\n",
        "print(\"Length of ingredient list:\", len(train_ingredients_list))\n",
        "print(\"Length of unit list:\", len(train_units_list))\n",
        "print(\"Length of quantity list:\", len(train_quantities_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGau4EgZCix"
      },
      "source": [
        "#### **4.4** *Top 10 Most Frequent Items* <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Define a function ***get_top_frequent_items*** to display top 10 most frequent items\n",
        "\n",
        "Here, item_list is used as a general parameter where you will call this function for ingredient and unit list\n",
        "\n",
        "Execute this function separately for top 10 most units and ingredients\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXc8h3H4ZOZ4"
      },
      "outputs": [],
      "source": [
        "# define a function get_top_frequent_items to get the top frequent items by using item_list, pos label and dataset_name(Training/Validation) and return top items\n",
        "def get_top_frequent_items(item_list, pos_label, dataset_name):\n",
        "    item_counts = Counter(item_list)\n",
        "    top_items = item_counts.most_common(10)\n",
        "    print(f\"\\nTop 10 Most Frequent {pos_label.capitalize()}s in {dataset_name} dataset:\")\n",
        "    for item, count in top_items:\n",
        "        print(f\"{item}: {count}\")\n",
        "    return top_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2jZCCf2MEke"
      },
      "outputs": [],
      "source": [
        "# get the top ingredients which are frequently seen in the recipe\n",
        "top_train_ingredients = get_top_frequent_items(train_ingredients_list, 'ingredient', dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wipghGXAMYQR"
      },
      "outputs": [],
      "source": [
        "# get the top units which are frequently seen in the recipe\n",
        "top_train_units = get_top_frequent_items(train_units_list, 'unit', dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hldpjOHaPVZ"
      },
      "source": [
        "#### **4.5** *Plot Top 10 most frequent items* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ImpWstybDP_"
      },
      "source": [
        "Define a function ***plot_top_items*** to plot a bar graph on top 10 most frequent items for units and ingredients\n",
        "\n",
        "Here, item_list is used as a general parameter where you will call this function for ingredient and unit list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmsq0L1vaxfc"
      },
      "outputs": [],
      "source": [
        "# define plot top items with parameters - top_item list, label to suggest whether its ingredient or unit, dataset_name\n",
        "def plot_top_items(top_items, label, dataset_name):\n",
        "    items, counts = zip(*top_items)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(items), y=list(counts), palette='viridis')\n",
        "    plt.title(f'Top 10 Most Frequent {label.capitalize()}s in {dataset_name} Dataset')\n",
        "    plt.xlabel(label.capitalize())\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHusCfkJ4suh"
      },
      "source": [
        "#### **4.6** *Perform EDA analysis* <font color = red>[5 marks]</font> <br>\n",
        "\n",
        "Plot the bar plots for ingredients and units and provide the insights for training dataset\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8seIqFKyYFmn"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent ingredients in training data\n",
        "plot_top_items(top_train_ingredients,'ingredient',dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbXAwiUkMtqT"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent units in training data\n",
        "plot_top_items(top_train_units,'unit',dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYh7zbJpCajJ"
      },
      "source": [
        "## **5** Exploratory Recipe Data Analysis on Validation Dataset (Optional)<font color = red> [0 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2wPIaOGCmk2"
      },
      "source": [
        "#### **5.1** *Execute EDA on Validation Dataset with insights (Optional)* <font color = red> [0 marks]</font> <br>\n",
        "Initialise the dataset_name as ***Validation*** and call the ***plot_top_items*** for top 10 ingredients and units in the recipe data\n",
        "Provide the insights for the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atSk0ChLPXHd"
      },
      "outputs": [],
      "source": [
        "# initialise the dataset_name\n",
        "val_dataset_name = 'Validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFPxheIuj1o8"
      },
      "outputs": [],
      "source": [
        "# use extract and validate tokens, categorise tokens, get top frequent items for ingredient list and unit list on validation dataframe\n",
        "val_ingredients, val_units = extract_and_validate_tokens(val_df, val_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNn-uwU-SAYc"
      },
      "outputs": [],
      "source": [
        "val_ingredients_list, val_units_list, val_quantities_list = categorize_tokens(val_ingredients, val_units)\n",
        "top_val_ingredients = get_top_frequent_items(val_ingredients_list, 'ingredients', val_dataset_name)\n",
        "top_val_units = get_top_frequent_items(val_units_list, 'unit', val_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikwox7ccMaU8"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent ingredients in validation data\n",
        "plot_top_items(top_val_ingredients,'ingredient',val_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QjVeMWpPwKO"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent units in training data\n",
        "plot_top_items(top_val_units,'unit',val_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvE92ait9GIS"
      },
      "source": [
        "## **6** Feature Extraction For CRF Model <font color = red>[30 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5Q_Lj09GIT"
      },
      "source": [
        "### **6.1** *Define a feature functions to take each token from recipe* <font color = red>[10 marks]</font>\n",
        "\n",
        "Define a function as ***word2features*** which takes a particular recipe and its index to work with all recipe input tokens and include custom key-value pairs.\n",
        "\n",
        "Also, use feature key-value pairs to mark the beginning and end of the sequence and to also check whether the word belongs to unit, quantity etc. Use keyword sets for unit and quantity for differentiating feature functions well. Also make use of relevant regex patterns on fractions, whole numbers etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyxmQ0PrhBra"
      },
      "source": [
        "##### **6.1.1** **Define keywords for unit and quantity and create a quantity pattern to work on fractions, numbers and decimals** <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Create sets for **unit_keywords** and ***quantity_keywords*** and include all the words relevant for measuring the ingredients such as cup, tbsp, tsp etc. and in quantity keywords, include words such as half, quarter etc.\n",
        "\n",
        "Also suggested to use regex pattern as ***quantity_pattern*** to work with quantity in any format such as fractions, numbers and decimals.\n",
        "\n",
        "Then, load the spacy model and process the entire sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhFUPxeth0KI"
      },
      "outputs": [],
      "source": [
        "# define unit and quantity keywords along with quantity pattern\n",
        "unit_keywords = train_units_list\n",
        "quantity_keywords = train_quantities_list\n",
        "# matches integers, decimals, and fractions\n",
        "quantity_pattern = re.compile(r'^\\d+(\\.\\d+)?(/[1-9]\\d*)?$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qmM8rw4VtJh"
      },
      "outputs": [],
      "source": [
        "# load spaCy model\n",
        "model = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrYD5tMNiFc-"
      },
      "source": [
        "##### **6.1.2** **Define feature functions for CRF** <font color = red>[7 marks]</font> <br>\n",
        "\n",
        "Define ***word2features*** function and use the parameters such as sentence and its indexing as ***sent*** and ***i*** for extracting token level features for CRF Training.\n",
        "Build ***features*** dictionary, also mark the beginning and end of the sequence and use the ***unit_keywords***, ***quantity_keywords*** and ***quantity_pattern*** for knowing the presence of quantity or unit in the tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAjf6j-dQtpr"
      },
      "source": [
        "While building ***features*** dictionary, include\n",
        "- ***Core Features*** - The core features of a token should capture its lexical\n",
        "and grammatical properties. Include attributes like the raw token, its lemma, part-of-speech tag, dependency relation, and shape, as well as indicators for whether it's a stop word, digit, or punctuation. The details of the features are given below:\n",
        "\n",
        "    - `bias` - Constant feature with a fixed value of 1.0 to aid model learning.\n",
        "    - `token` - The lowercase form of the current token.\n",
        "    - `lemma` - The lowercase lemma (base form) of the token.\n",
        "    - `pos_tag` - Part-of-speech (POS) tag of the token.\n",
        "    - `tag` - Detailed POS tag of the token.\n",
        "    - `dep` - Dependency relation of the token in the sentence.\n",
        "    - `shape` - Shape of the token (e.g., \"Xxx\" for \"Milk\").\n",
        "    - `is_stop` - Boolean indicating if the token is a stopword.\n",
        "    - `is_digit` - Boolean indicating if the token consists of only digits.\n",
        "    - `has_digit` - Boolean indicating if the token contains at least one digit.\n",
        "    - `has_alpha` - Boolean indicating if the token contains at least one alphabetic character.\n",
        "    - `hyphenated` - Boolean indicating if the token contains a hyphen (-).\n",
        "    - `slash_present` - Boolean indicating if the token contains a slash (/).\n",
        "    - `is_title` - Boolean indicating if the token starts with an uppercase letter.\n",
        "    - `is_upper` - Boolean indicating if the token is fully uppercase.\n",
        "    - `is_punct` - Boolean indicating if the token is a punctuation mark.\n",
        "\n",
        "- ***Improved Quantity and Unit Detection*** - Use key-value pairs to mark the presence of quantities and units in the features dictionary. Utilise the unit_keywords, quantity_keywords, and quantity_pattern to identify and flag these elements. The details of the features are given below:\n",
        "\n",
        "    - `is_quantity` - Boolean indicating if the token matches a quantity pattern or keyword.\n",
        "    - `is_unit` - Boolean indicating if the token is a known measurement unit.\n",
        "    - `is_numeric` - Boolean indicating if the token matches a numeric pattern.\n",
        "    - `is_fraction` - Boolean indicating if the token represents a fraction (e.g., 1/2).\n",
        "    - `is_decimal` - Boolean indicating if the token represents a decimal number (e.g., 3.14).\n",
        "    - `preceding_word` - The previous token in the sentence, if available.\n",
        "    - `following_word` - The next token in the sentence, if available.\n",
        "\n",
        "- ***Contextual Features*** - Incorporate contextual information by adding features for the preceding and following tokens. Include indicators like BOS and EOS to mark the beginning and end of the sequence, and utilise unit_keywords, quantity_keywords, and quantity_pattern to identify the types of neighboring tokens. The features are given below:\n",
        "\n",
        "    - `prev_token` - The lowercase form of the previous token.\n",
        "    - `prev_is_quantity` - Boolean indicating if the previous token is a quantity.\n",
        "    - `prev_is_digit` - Boolean indicating if the previous token is a digit.\n",
        "    - `BOS` - Boolean indicating if the token is at the beginning of the sentence.\n",
        "    - `next_token` - The lowercase form of the next token.\n",
        "    - `next_is_unit` - Boolean indicating if the next token is a unit.\n",
        "    - `next_is_ingredient` - Boolean indicating if the next token is not a unit or quantity.\n",
        "    - `EOS` - Boolean indicating if the token is at the end of the sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRU7efTF9GIW"
      },
      "outputs": [],
      "source": [
        "# define word2features for processing each token in the sentence sent by using index i.\n",
        "# use your own feature functions\n",
        "def word2features(sent, pos):\n",
        "    word = sent[pos]\n",
        "    features = {\n",
        "        # --- Core Features ---\n",
        "        'bias': 1.0,\n",
        "        'token': word.lower(),\n",
        "        'lemma': model(word)[0].lemma_.lower(),\n",
        "        'pos_tag': model(word)[0].pos_,\n",
        "        'tag': model(word)[0].tag_,\n",
        "        'dep': model(word)[0].dep_,\n",
        "        'shape': re.sub(r'[0-9]', '0', re.sub(r'[A-Z]', 'X', re.sub(r'[a-z]', 'x', word))),\n",
        "        'is_stop': model(word)[0].is_stop,\n",
        "        'is_digit': word.isdigit(),\n",
        "        'has_digit': any(char.isdigit() for char in word),\n",
        "        'has_alpha': any(char.isalpha() for char in word),\n",
        "        'hyphenated': '-' in word,\n",
        "        'slash_present': '/' in word,\n",
        "        'is_title': word.istitle(),\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_punct': model(word)[0].is_punct,\n",
        "\n",
        "        # --- Improved Quantity & Unit Detection ---\n",
        "        'is_quantity': word.lower() in quantity_keywords or (quantity_pattern.match(word) is not None), # Ensure match returns boolean\n",
        "        'is_unit': word.lower() in unit_keywords,\n",
        "        'is_numeric': word.replace('.', '', 1).isdigit(),\n",
        "        'is_fraction': '/' in word and (quantity_pattern.match(word) is not None), # Ensure match returns boolean\n",
        "        'is_decimal': '.' in word and word.replace('.', '', 1).isdigit(),\n",
        "        'preceding_word': sent[pos - 1] if pos > 0 else '',\n",
        "        'following_word': sent[pos + 1] if pos < len(sent) - 1 else '',\n",
        "\n",
        "        # --- Contextual Features ---\n",
        "        'prev_token': sent[pos - 1].lower() if pos > 0 else '',\n",
        "        'prev_is_quantity': (sent[pos - 1].lower() in quantity_keywords or (quantity_pattern.match(sent[pos - 1]) is not None)) if pos > 0 else False, # Ensure boolean\n",
        "        'prev_is_digit': sent[pos - 1].isdigit() if pos > 0 else False, # Ensure boolean\n",
        "        'BOS': pos == 0,\n",
        "        'next_token': sent[pos + 1].lower() if pos < len(sent) - 1 else '',\n",
        "        'next_is_unit': (sent[pos + 1].lower() in unit_keywords if pos < len(sent) - 1 else False), # Ensure boolean\n",
        "        'next_is_ingredient': ((sent[pos + 1].lower() not in unit_keywords and (sent[pos + 1].lower() not in quantity_keywords and (quantity_pattern.match(sent[pos + 1]) is not None))) if pos < len(sent) - 1 else False), # Ensure boolean\n",
        "        'EOS': pos == len(sent) - 1\n",
        "    }\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJm2nUw0998s"
      },
      "source": [
        "### **6.2** *Preparation of Recipe level features* <font color = red>[2 marks]</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL19ooQejA5z"
      },
      "source": [
        "##### **6.2.1** **Define function to work on all the recipes and call word2features for each recipe** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define ***sent2features*** function and inputs ***sent*** as a parameter and correctly generate feature functions for each token present in the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQEifz-9GIW"
      },
      "outputs": [],
      "source": [
        "# define sent2features by working on each token in the sentence and correctly generate dictionaries for features\n",
        "def sent2features(sent):\n",
        "  return [word2features(sent, i) for i in range(len(sent))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOK0t3c6-RiV"
      },
      "source": [
        "### **6.3** *Convert X_train, X_val, y_train and y_val into train and validation feature sets and labels* <font color = red>[6 marks]</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tsd50b_nX0J"
      },
      "source": [
        "##### **6.3.1** **Convert recipe into feature functions by using X_train and X_val** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***X_train_features*** and ***X_val_features*** as list to include the feature functions for each recipe present in training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bVPGPa39GIW"
      },
      "outputs": [],
      "source": [
        "# Convert input sentences into feature sets by taking training and validation dataset as X_train_features and X_val_features\n",
        "X_train_features = [sent2features(sent) for sent in X_train]\n",
        "X_val_features = [sent2features(sent) for sent in X_val]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcwmwXn-n6cs"
      },
      "source": [
        "##### **6.3.2** **Convert lables of y_train and y_val into list** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***y_train_labels*** and ***y_val_labels*** by using the list of y_train and y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiGgP3O6nfPg"
      },
      "outputs": [],
      "source": [
        "# Convert labels into list as y_train_labels and y_val_labels\n",
        "y_train_labels = y_train\n",
        "y_val_labels = y_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c-kjqtaoZvb"
      },
      "source": [
        "##### **6.3.3** **Print the length of val and train features and labels** <font color = red>[2 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWId2Nn0okMV"
      },
      "outputs": [],
      "source": [
        "# print the length of train features and labels\n",
        "print(\"Length of X_train_features:\", len(X_train_features))\n",
        "print(\"Length of y_train_labels:\", len(y_train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAt_m_LubRvn"
      },
      "outputs": [],
      "source": [
        "# print the length of validation features and labels\n",
        "print(\"Length of X_val_features:\", len(X_val_features))\n",
        "print(\"Length of y_val_labels:\", len(y_val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZffFBH-pVhx"
      },
      "source": [
        "### **6.4** *Applying weights to feature sets* <font color = red>[12 marks]</font> <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goh_fX-6pqhN"
      },
      "source": [
        "##### **6.4.1** **Flatten the labels of y_train** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***y_train_flat*** to flatten the structure of nested y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adLWfYn_p3gM"
      },
      "outputs": [],
      "source": [
        "# Flatten labels in y_train\n",
        "y_train_flat = flatten_list(y_train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk9UMBrbp9dp"
      },
      "source": [
        "##### **6.4.2** **Count the labels present in training target dataset** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***label_counts*** to count the frequencies of labels present in y_train_flat and retrieve the total samples by using the values of label_counts as ***total_samples***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kiu8jckqZSH"
      },
      "outputs": [],
      "source": [
        "# Count label frequencies as label_counts and total_samples as getting the summation of values of label_counts\n",
        "label_counts = Counter(y_train_flat)\n",
        "total_samples = sum(label_counts.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aCmDsZYqYA-"
      },
      "source": [
        "##### **6.4.3** **Compute weight_dict by using inverse frequency method for label weights** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "- Create ***weight_dict*** as dictionary with label and its inverse frequency count in ***label_counts***\n",
        "\n",
        "- Penalise ingredient label in the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpbEAZ3zqxEo"
      },
      "outputs": [],
      "source": [
        "# Compute class weights (inverse frequency method) by considering total_samples and label_counts\n",
        "weight_dict = {label: total_samples / count for label, count in label_counts.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hns3HbujXESs"
      },
      "outputs": [],
      "source": [
        "# penalise ingredient label\n",
        "weight_dict['ingredient'] = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8TdHMlPrhh8"
      },
      "source": [
        "##### **6.4.4** **Extract features along with class weights** <font color = red>[4 marks]</font> <br>\n",
        "\n",
        "Define a function ***extract_features_with_class_weights*** to work with training and validation datasets and extract features by applying class weights\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1km6GR4TjXPX"
      },
      "outputs": [],
      "source": [
        "# Apply weights to feature extraction in extract_features_with_class_weights by using parameters such as X (input tokens), y(labels) and weight_dict (Class weights)\n",
        "def extract_features_with_class_weights(X_features, y_labels, weight_dict):\n",
        "    X_weighted_features = []\n",
        "    y_weighted_labels = [] # Keep track of labels as well, although they aren't modified\n",
        "\n",
        "    for sentence_features, labels in zip(X_features, y_labels):\n",
        "        weighted_sentence_features = []\n",
        "        for i in range(len(sentence_features)):\n",
        "            features = sentence_features[i].copy() # Create a copy to avoid modifying the original features\n",
        "            label = labels[i]\n",
        "            features['class_weight'] = weight_dict.get(label, 1.0) # Add class weight feature\n",
        "            weighted_sentence_features.append(features)\n",
        "        X_weighted_features.append(weighted_sentence_features)\n",
        "        y_weighted_labels.append(labels)\n",
        "\n",
        "    return X_weighted_features, y_weighted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51ABmKwKsaiz"
      },
      "source": [
        "##### **6.4.5** **Execute extract_features_with_class_weights on training and validation datasets** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***X_train_weighted_features*** and ***X_val_weighted_features*** for extracting training and validation features along with their weights by calling ***extract_features_with_class_weights*** on the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XUFFnm5sYE6"
      },
      "outputs": [],
      "source": [
        "# Apply manually computed class weights\n",
        "X_train_weighted_features, y_train_weighted_labels = extract_features_with_class_weights(X_train_features, y_train_labels, weight_dict)\n",
        "X_val_weighted_features, y_val_weighted_labels = extract_features_with_class_weights(X_val_features, y_val_labels, weight_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aah9bFDlAuzI"
      },
      "source": [
        "## **7** Model Building and Training <font color = red>[10 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axrvWR9TAuzJ"
      },
      "source": [
        "### **7.1** *Initialise the CRF model and train it* <font color = red>[5 marks]</font>\n",
        "Train the CRF model with the specified hyperparameters such as\n",
        "\n",
        "### CRF Model Hyperparameters Explanation\n",
        "\n",
        "| Parameter                  | Description |\n",
        "|----------------------------|-------------|\n",
        "| **algorithm='lbfgs'**      | Optimisation algorithm used for training. `lbfgs` (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton optimisation method. |\n",
        "| **c1=0.5**                | L1 regularisation term to control sparsity in feature weights. Helps in feature selection. |\n",
        "| **c2=1.0**                | L2 regularisation term to prevent overfitting by penalising large weights. |\n",
        "| **max_iterations=100**     | Maximum number of iterations for model training. Higher values allow more convergence but increase computation time. |\n",
        "| **all_possible_transitions=True** | Ensures that all possible state transitions are considered in training, making the model more robust. |\n",
        "\n",
        "Use weight_dict for training CRF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jig2J_n1AuzM"
      },
      "outputs": [],
      "source": [
        "# initialise CRF model with the specified hyperparameters and use weight_dict\n",
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs',\n",
        "                           c1=0.5,\n",
        "                           c2=1.0,\n",
        "                           max_iterations=100,\n",
        "                           all_possible_transitions=True)\n",
        "# train the CRF model with the weighted training data\n",
        "crf.fit(X_train_weighted_features, y_train_weighted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDLwvYqOF6m_"
      },
      "source": [
        "### **7.2** *Evaluation of Training Dataset using CRF model* <font color = red>[4 marks]</font>\n",
        "Evaluate on training dataset using CRF by using flat classification report and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us57jWSQ6laL"
      },
      "outputs": [],
      "source": [
        "# evaluate on the training dataset\n",
        "y_pred_train = crf.predict(X_train_weighted_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNGZnd-D6oq3"
      },
      "outputs": [],
      "source": [
        "# specify the flat classification report by using training data for evaluation\n",
        "print(metrics.flat_classification_report(y_train_weighted_labels, y_pred_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the f1 score using the train data\n",
        "metrics.flat_f1_score(y_train_weighted_labels, y_pred_train, average='weighted')"
      ],
      "metadata": {
        "id": "jaxhJwbV1kM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqP9WBvJ63qm"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix on training datset\n",
        "print(confusion_matrix(y_train_weighted_labels, y_pred_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yps2-XscGuHc"
      },
      "source": [
        "### **7.3** *Save the CRF model* <font color = red>[1 marks]</font>\n",
        "Save the CRF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAYDLatcGzEN"
      },
      "outputs": [],
      "source": [
        "# dump the model using joblib as crf_model.pkl\n",
        "joblib.dump(crf, 'crf_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agM32oUlBo1K"
      },
      "source": [
        "## **8** Prediction and Model Evaluation <font color = red>[3 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5BYmkTrBo1L"
      },
      "source": [
        "### **8.1** *Predict and Evaluate the CRF model on validation set* <font color = red>[3 marks]</font>\n",
        "Evaluate the metrics for CRF model by using flat classification report and confusion matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhH6Sp8tBo1M"
      },
      "outputs": [],
      "source": [
        "# predict the crf model on validation dataset\n",
        "y_pred_val = crf.predict(X_val_weighted_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMktt_w1kovB"
      },
      "outputs": [],
      "source": [
        "# specify flat classification report\n",
        "print(metrics.flat_classification_report(y_val_weighted_labels, y_pred_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the f1 score using the test data\n",
        "metrics.flat_f1_score(y_val_weighted_labels, y_pred_val, average='weighted')"
      ],
      "metadata": {
        "id": "HQ_mlyWJ1WgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI2tUBRRk4jK"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix on validation dataset\n",
        "print(confusion_matrix(y_val_weighted_labels, y_pred_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pD6hD3NEV3q"
      },
      "source": [
        "## **9** Error Analysis on Validation Data <font color = red>[10 marks]</font> <br>\n",
        "Investigate misclassified samples in validation dataset and provide the insights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9tUvjrzFjib"
      },
      "source": [
        "### **9.1** *Investigate misclassified samples in validation dataset* <font color = red>[8 marks]</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb15uObqxKe4"
      },
      "source": [
        "##### **9.1.1** Flatten the labels of validation data and initialise error data <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Flatten the true and predicted labels and initialise the error data as ***error_data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbgYAjd-UzkI"
      },
      "outputs": [],
      "source": [
        "# flatten Labels and Initialise Error Data\n",
        "y_val_flat = flatten_list(y_val_weighted_labels)\n",
        "y_pred_val_flat = flatten_list(y_pred_val)\n",
        "X_val_weighted_features_flat = flatten_list(X_val_weighted_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9foWfdXHOg"
      },
      "source": [
        "##### **9.1.2** Iterate the validation data and collect Error Information<font color = red> [2 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Iterate through validation data (X_val, y_val_labels, y_pred_val) and compare true vs. predicted labels. Collect error details, including surrounding context, previous/next tokens, and class weights, then store them in error_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VKLc1s0U0yY"
      },
      "outputs": [],
      "source": [
        "# iterate and collect Error Information\n",
        "# get previous and next tokens with handling for boundary cases\n",
        "error_data = []\n",
        "for i in range(len(y_val_flat)):\n",
        "    if y_val_flat[i] != y_pred_val_flat[i]:\n",
        "      error_data.append({\n",
        "          'token': X_val_weighted_features_flat[i]['token'],\n",
        "          'previous_token': X_val_weighted_features_flat[i - 1]['token'] if i > 0 else '',\n",
        "          'next_token': X_val_weighted_features_flat[i + 1]['token'] if i < len(X_val) - 1 else '',\n",
        "          'true_label': y_val_flat[i],\n",
        "          'predicted_label': y_pred_val_flat[i],\n",
        "          'context': f\"{X_val_weighted_features_flat[i - 1]['token']} {X_val_weighted_features_flat[i]['token']} {X_val_weighted_features_flat[i + 1]['token']}\"\n",
        "      })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_R8CCAFZSzF"
      },
      "source": [
        "##### **9.1.3** Create dataframe from error_data and print overall accuracy <font color = red>[1 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Change error_data into dataframe and then use it to illustrate the overall accuracy of validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUffRP7XU3YC"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame and Print Overall Accuracy\n",
        "error_df = pd.DataFrame(error_data)\n",
        "error_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OUYHFmgZhgJ"
      },
      "source": [
        "##### **9.1.4** Analyse errors by label type<font color = red> [3 marks]</font> <br>\n",
        "Analyse errors found in the validation data by each label and display their class weights along with accuracy and also display the error dataframe with token,  previous token, next token, true label, predicted label and context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "collapsed": true,
        "id": "zu8CtjU6WR9l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "e879b7d5-82e1-4e0a-bcc2-a781134acdcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights and Accuracy per Label:\n",
            "Label: ingredient, Class Weight: 1.00, Accuracy: 1.00\n",
            "Label: quantity, Class Weight: 7.26, Accuracy: 0.99\n",
            "Label: unit, Class Weight: 8.77, Accuracy: 0.99\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Error Analysis DataFrame"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    token previous_token next_token true_label predicted_label                 context\n",
              "0      is            pur              quantity            unit                pur is 2\n",
              "1     for            oil              quantity            unit        oil for kneading\n",
              "2      to             10                  unit        quantity                10 to 12\n",
              "3       a          haldi                  unit        quantity           haldi a pinch\n",
              "4   pinch            dal              quantity            unit    dal pinch asafoetida\n",
              "5  cloves       tomatoes              quantity            unit  tomatoes cloves garlic"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b30227c3-b807-4618-8ef1-3502c5cb376f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>previous_token</th>\n",
              "      <th>next_token</th>\n",
              "      <th>true_label</th>\n",
              "      <th>predicted_label</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is</td>\n",
              "      <td>pur</td>\n",
              "      <td></td>\n",
              "      <td>quantity</td>\n",
              "      <td>unit</td>\n",
              "      <td>pur is 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>for</td>\n",
              "      <td>oil</td>\n",
              "      <td></td>\n",
              "      <td>quantity</td>\n",
              "      <td>unit</td>\n",
              "      <td>oil for kneading</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>to</td>\n",
              "      <td>10</td>\n",
              "      <td></td>\n",
              "      <td>unit</td>\n",
              "      <td>quantity</td>\n",
              "      <td>10 to 12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a</td>\n",
              "      <td>haldi</td>\n",
              "      <td></td>\n",
              "      <td>unit</td>\n",
              "      <td>quantity</td>\n",
              "      <td>haldi a pinch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pinch</td>\n",
              "      <td>dal</td>\n",
              "      <td></td>\n",
              "      <td>quantity</td>\n",
              "      <td>unit</td>\n",
              "      <td>dal pinch asafoetida</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cloves</td>\n",
              "      <td>tomatoes</td>\n",
              "      <td></td>\n",
              "      <td>quantity</td>\n",
              "      <td>unit</td>\n",
              "      <td>tomatoes cloves garlic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b30227c3-b807-4618-8ef1-3502c5cb376f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b30227c3-b807-4618-8ef1-3502c5cb376f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b30227c3-b807-4618-8ef1-3502c5cb376f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c9963f66-88bd-4620-ba38-8fe2747b8217\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9963f66-88bd-4620-ba38-8fe2747b8217')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c9963f66-88bd-4620-ba38-8fe2747b8217 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(error_df[['token', 'previous_token', 'next_token', 'true_label', 'predicted_label', 'context']])\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"is\",\n          \"for\",\n          \"cloves\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"previous_token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"pur\",\n          \"oil\",\n          \"tomatoes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"next_token\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"true_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"unit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"quantity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"pur is 2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Analyse errors found in the validation data by each label\n",
        "# and display their class weights along with accuracy\n",
        "# and display the error dataframe with token, previous token, next token, true label, predicted label and context\n",
        "\n",
        "# Group errors by true label\n",
        "errors_by_label = error_df.groupby('true_label').size().reset_index(name='error_count')\n",
        "\n",
        "# Calculate total instances for each label in the validation set\n",
        "val_label_counts = Counter(y_val_flat)\n",
        "total_val_samples = sum(val_label_counts.values())\n",
        "\n",
        "# Calculate accuracy per label\n",
        "label_accuracy = {label: 1 - (errors_by_label[errors_by_label['true_label'] == label]['error_count'].sum() / val_label_counts[label]) if label in val_label_counts and val_label_counts[label] > 0 else 0 for label in all_unique_labels}\n",
        "\n",
        "# Display class weights and accuracy per label\n",
        "print(\"Class Weights and Accuracy per Label:\")\n",
        "for label in all_unique_labels:\n",
        "    weight = weight_dict.get(label, 1.0)\n",
        "    accuracy = label_accuracy.get(label, 0)\n",
        "    print(f\"Label: {label}, Class Weight: {weight:.2f}, Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display the error dataframe with relevant columns\n",
        "display(Markdown(\"### Error Analysis DataFrame\"))\n",
        "display(error_df[['token', 'previous_token', 'next_token', 'true_label', 'predicted_label', 'context']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3n74kVvEV3q"
      },
      "source": [
        "\\### **9.2** *Provide insights from the validation dataset* <font color = red>[2 marks]</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWZdf1O_vWnD"
      },
      "source": [
        "Only six tokens were inaccurately predicted; the remaining predictions appear correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUjFPBMxH20n"
      },
      "source": [
        "## **10** Conclusion (Optional) <font color = red>[0 marks]</font> <br>\n",
        "\n",
        "Write your findings and conclusion."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}